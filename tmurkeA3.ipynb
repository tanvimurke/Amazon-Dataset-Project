{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183254c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c89213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Open the JSON file and read the data in chunks\n",
    "json_reader = pd.read_json('downloads/amazon/Home_and_Kitchen.json', lines=True, chunksize=10000)\n",
    "\n",
    "# Create an empty list to store the sampled chunks\n",
    "sampled_chunks = []\n",
    "\n",
    "# Loop through the chunks and randomly select a subset\n",
    "for chunk in json_reader:\n",
    "    # Calculate the number of rows to sample\n",
    "    num_rows = int(len(chunk) * 0.01)\n",
    "    \n",
    "    # If the number of rows to sample is greater than the chunk size, set it to the chunk size\n",
    "    if num_rows >= len(chunk):\n",
    "        num_rows = len(chunk)\n",
    "    \n",
    "    # Randomly select the rows\n",
    "    random_indices = random.sample(range(len(chunk)), num_rows)\n",
    "    \n",
    "    # Append the sampled rows to the list\n",
    "    sampled_chunks.append(chunk.iloc[random_indices])\n",
    "    \n",
    "# Concatenate the sampled chunks into a single dataframe\n",
    "data = pd.concat(sampled_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ca5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwords and punctuation to remove\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9be7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba02c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def preprocess(text):\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # remove stopwords and words with length less than 3\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    # lemmatize words\n",
    "    tokens = [lemma.lemmatize(word) for word in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9d4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanvimurke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tanvimurke/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanvimurke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download stopwords, wordnet, punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777836ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for index, row in data.iterrows():\n",
    "    if 'reviewText' in row and isinstance(row['reviewText'], str):\n",
    "        documents.append(preprocess(row['reviewText']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e28d8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and bag of words\n",
    "dictionary = Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b6c469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10, alpha='auto', eta='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1f6ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score (LDA): 0.5357118632614479\n"
     ]
    }
   ],
   "source": [
    "# evaluate coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score (LDA):', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da603e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"little\" + 0.023*\"fit\" + 0.020*\"put\" + 0.019*\"small\" + 0.018*\"cute\" + 0.014*\"piece\" + 0.014*\"need\" + 0.013*\"hold\" + 0.012*\"top\" + 0.012*\"doesnt\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"unit\" + 0.020*\"blanket\" + 0.017*\"heat\" + 0.016*\"instruction\" + 0.014*\"turn\" + 0.012*\"low\" + 0.010*\"name\" + 0.010*\"oven\" + 0.009*\"feature\" + 0.009*\"minute\"\n",
      "Topic: 2 \n",
      "Words: 0.067*\"one\" + 0.034*\"time\" + 0.022*\"get\" + 0.021*\"bought\" + 0.020*\"used\" + 0.017*\"year\" + 0.016*\"two\" + 0.015*\"still\" + 0.015*\"day\" + 0.015*\"pillow\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"would\" + 0.018*\"like\" + 0.017*\"gift\" + 0.016*\"item\" + 0.015*\"picture\" + 0.014*\"got\" + 0.013*\"came\" + 0.013*\"set\" + 0.011*\"even\" + 0.011*\"received\"\n",
      "Topic: 4 \n",
      "Words: 0.028*\"cut\" + 0.025*\"food\" + 0.021*\"bright\" + 0.021*\"container\" + 0.021*\"bowl\" + 0.019*\"pictured\" + 0.015*\"oil\" + 0.015*\"steel\" + 0.015*\"blade\" + 0.014*\"jar\"\n",
      "Topic: 5 \n",
      "Words: 0.060*\"glass\" + 0.051*\"cup\" + 0.051*\"water\" + 0.040*\"lid\" + 0.040*\"coffee\" + 0.033*\"mug\" + 0.033*\"bottle\" + 0.031*\"hot\" + 0.022*\"tea\" + 0.020*\"drink\"\n",
      "Topic: 6 \n",
      "Words: 0.075*\"great\" + 0.043*\"look\" + 0.040*\"good\" + 0.040*\"nice\" + 0.039*\"well\" + 0.037*\"quality\" + 0.037*\"product\" + 0.033*\"work\" + 0.026*\"color\" + 0.026*\"like\"\n",
      "Topic: 7 \n",
      "Words: 0.063*\"rug\" + 0.045*\"clock\" + 0.035*\"battery\" + 0.029*\"floor\" + 0.028*\"vacuum\" + 0.021*\"dog\" + 0.016*\"cat\" + 0.014*\"brush\" + 0.014*\"power\" + 0.013*\"hair\"\n",
      "Topic: 8 \n",
      "Words: 0.058*\"soft\" + 0.054*\"bed\" + 0.047*\"sheet\" + 0.034*\"cover\" + 0.028*\"mattress\" + 0.021*\"fan\" + 0.020*\"air\" + 0.020*\"night\" + 0.018*\"tree\" + 0.017*\"washed\"\n",
      "Topic: 9 \n",
      "Words: 0.111*\"love\" + 0.058*\"use\" + 0.049*\"perfect\" + 0.037*\"easy\" + 0.034*\"size\" + 0.031*\"make\" + 0.015*\"table\" + 0.014*\"clean\" + 0.012*\"hand\" + 0.011*\"kitchen\"\n"
     ]
    }
   ],
   "source": [
    "# print topics and their top words\n",
    "for index, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c09fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train other LDA model\n",
    "lda_model2 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=10, alpha='auto', eta='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b833458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score (LDA): 0.48086689658163817\n"
     ]
    }
   ],
   "source": [
    "# evaluate coherence score\n",
    "coherence_model_lda2 = CoherenceModel(model=lda_model2, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda2 = coherence_model_lda2.get_coherence()\n",
    "print('Coherence Score (LDA):', coherence_lda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a523a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.072*\"curtain\" + 0.059*\"pan\" + 0.045*\"shower\" + 0.032*\"steel\" + 0.030*\"pot\" + 0.028*\"oil\" + 0.027*\"cake\" + 0.026*\"stick\" + 0.025*\"stainless\" + 0.025*\"cooking\"\n",
      "Topic: 1 \n",
      "Words: 0.071*\"cup\" + 0.071*\"water\" + 0.055*\"coffee\" + 0.046*\"mug\" + 0.043*\"hot\" + 0.031*\"tea\" + 0.027*\"drink\" + 0.026*\"cold\" + 0.024*\"pictured\" + 0.024*\"filter\"\n",
      "Topic: 2 \n",
      "Words: 0.106*\"review\" + 0.035*\"read\" + 0.031*\"instruction\" + 0.025*\"birthday\" + 0.022*\"discount\" + 0.021*\"included\" + 0.021*\"shown\" + 0.019*\"opinion\" + 0.019*\"tray\" + 0.018*\"display\"\n",
      "Topic: 3 \n",
      "Words: 0.073*\"money\" + 0.070*\"worth\" + 0.056*\"smell\" + 0.047*\"photo\" + 0.042*\"bad\" + 0.033*\"nothing\" + 0.028*\"waste\" + 0.027*\"assemble\" + 0.025*\"pay\" + 0.022*\"wrong\"\n",
      "Topic: 4 \n",
      "Words: 0.166*\"put\" + 0.149*\"cute\" + 0.087*\"table\" + 0.074*\"together\" + 0.061*\"awesome\" + 0.044*\"kid\" + 0.033*\"lovely\" + 0.025*\"adorable\" + 0.021*\"putting\" + 0.018*\"compliment\"\n",
      "Topic: 5 \n",
      "Words: 0.189*\"beautiful\" + 0.095*\"arrived\" + 0.076*\"bag\" + 0.052*\"frame\" + 0.049*\"pleased\" + 0.025*\"packaged\" + 0.019*\"condition\" + 0.018*\"shipped\" + 0.018*\"detail\" + 0.018*\"decor\"\n",
      "Topic: 6 \n",
      "Words: 0.036*\"dont\" + 0.026*\"piece\" + 0.023*\"also\" + 0.021*\"top\" + 0.020*\"doesnt\" + 0.020*\"enough\" + 0.020*\"need\" + 0.017*\"side\" + 0.016*\"make\" + 0.014*\"hand\"\n",
      "Topic: 7 \n",
      "Words: 0.105*\"rug\" + 0.057*\"bright\" + 0.049*\"floor\" + 0.047*\"vacuum\" + 0.047*\"area\" + 0.039*\"mat\" + 0.036*\"dog\" + 0.032*\"cleaning\" + 0.026*\"cat\" + 0.023*\"brush\"\n",
      "Topic: 8 \n",
      "Words: 0.341*\"love\" + 0.088*\"gift\" + 0.059*\"loved\" + 0.034*\"knife\" + 0.033*\"absolutely\" + 0.032*\"christmas\" + 0.027*\"friend\" + 0.026*\"son\" + 0.019*\"fun\" + 0.016*\"wife\"\n",
      "Topic: 9 \n",
      "Words: 0.084*\"lid\" + 0.053*\"towel\" + 0.039*\"dish\" + 0.038*\"paper\" + 0.034*\"plate\" + 0.034*\"thanks\" + 0.031*\"dishwasher\" + 0.028*\"purpose\" + 0.025*\"lunch\" + 0.024*\"spoon\"\n",
      "Topic: 10 \n",
      "Words: 0.043*\"one\" + 0.029*\"use\" + 0.022*\"time\" + 0.019*\"bought\" + 0.019*\"get\" + 0.015*\"much\" + 0.014*\"used\" + 0.013*\"even\" + 0.012*\"got\" + 0.011*\"two\"\n",
      "Topic: 11 \n",
      "Words: 0.085*\"glass\" + 0.075*\"plastic\" + 0.065*\"cheap\" + 0.047*\"bottle\" + 0.046*\"white\" + 0.044*\"thin\" + 0.039*\"wood\" + 0.031*\"black\" + 0.028*\"hole\" + 0.027*\"wine\"\n",
      "Topic: 12 \n",
      "Words: 0.061*\"great\" + 0.037*\"like\" + 0.036*\"look\" + 0.033*\"good\" + 0.033*\"nice\" + 0.032*\"well\" + 0.030*\"quality\" + 0.030*\"product\" + 0.027*\"work\" + 0.024*\"perfect\"\n",
      "Topic: 13 \n",
      "Words: 0.062*\"received\" + 0.057*\"box\" + 0.052*\"bed\" + 0.034*\"amazon\" + 0.034*\"shipping\" + 0.030*\"return\" + 0.030*\"order\" + 0.029*\"seller\" + 0.028*\"broken\" + 0.022*\"company\"\n",
      "Topic: 14 \n",
      "Words: 0.045*\"making\" + 0.043*\"food\" + 0.041*\"party\" + 0.039*\"container\" + 0.039*\"bowl\" + 0.022*\"mold\" + 0.019*\"pop\" + 0.019*\"handy\" + 0.018*\"install\" + 0.014*\"fruit\"\n",
      "Topic: 15 \n",
      "Words: 0.120*\"pillow\" + 0.061*\"wall\" + 0.043*\"case\" + 0.043*\"print\" + 0.042*\"blanket\" + 0.038*\"screw\" + 0.035*\"ice\" + 0.035*\"door\" + 0.026*\"hanging\" + 0.023*\"sleep\"\n",
      "Topic: 16 \n",
      "Words: 0.146*\"room\" + 0.053*\"space\" + 0.050*\"hang\" + 0.038*\"tree\" + 0.036*\"shelf\" + 0.035*\"hook\" + 0.035*\"holder\" + 0.031*\"window\" + 0.029*\"storage\" + 0.026*\"counter\"\n",
      "Topic: 17 \n",
      "Words: 0.112*\"light\" + 0.043*\"cool\" + 0.043*\"clock\" + 0.041*\"unit\" + 0.034*\"bathroom\" + 0.034*\"fan\" + 0.033*\"air\" + 0.031*\"warm\" + 0.026*\"turn\" + 0.025*\"night\"\n",
      "Topic: 18 \n",
      "Words: 0.083*\"thank\" + 0.041*\"jar\" + 0.039*\"ordering\" + 0.033*\"paint\" + 0.033*\"pack\" + 0.031*\"zipper\" + 0.029*\"art\" + 0.026*\"sink\" + 0.025*\"panel\" + 0.018*\"fridge\"\n",
      "Topic: 19 \n",
      "Words: 0.235*\"easy\" + 0.094*\"clean\" + 0.084*\"super\" + 0.079*\"best\" + 0.060*\"fast\" + 0.054*\"ever\" + 0.048*\"amazing\" + 0.037*\"delivery\" + 0.031*\"simple\" + 0.027*\"quick\"\n"
     ]
    }
   ],
   "source": [
    "# print topics and their top words\n",
    "for index, topic in lda_model2.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ada5e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train other LDA model\n",
    "lda_model3 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=5, alpha='auto', eta='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f21ec349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score (LDA): 0.4694422209436747\n"
     ]
    }
   ],
   "source": [
    "# evaluate coherence score\n",
    "coherence_model_lda3 = CoherenceModel(model=lda_model3, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda3 = coherence_model_lda3.get_coherence()\n",
    "print('Coherence Score (LDA):', coherence_lda3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "003b77ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.094*\"cup\" + 0.094*\"water\" + 0.073*\"coffee\" + 0.062*\"mug\" + 0.056*\"hot\" + 0.041*\"tea\" + 0.036*\"wine\" + 0.029*\"drink\" + 0.025*\"filter\" + 0.023*\"machine\"\n",
      "Topic: 1 \n",
      "Words: 0.048*\"black\" + 0.039*\"show\" + 0.039*\"bigger\" + 0.036*\"plate\" + 0.029*\"pattern\" + 0.027*\"true\" + 0.023*\"dark\" + 0.022*\"okay\" + 0.022*\"spot\" + 0.021*\"brown\"\n",
      "Topic: 2 \n",
      "Words: 0.219*\"expected\" + 0.053*\"heat\" + 0.034*\"zipper\" + 0.032*\"burn\" + 0.031*\"temperature\" + 0.028*\"grinder\" + 0.022*\"bath\" + 0.019*\"memory\" + 0.019*\"guy\" + 0.019*\"lol\"\n",
      "Topic: 3 \n",
      "Words: 0.238*\"perfect\" + 0.138*\"gift\" + 0.093*\"loved\" + 0.068*\"sheet\" + 0.057*\"daughter\" + 0.049*\"christmas\" + 0.042*\"friend\" + 0.041*\"son\" + 0.037*\"husband\" + 0.034*\"cool\"\n",
      "Topic: 4 \n",
      "Words: 0.162*\"good\" + 0.162*\"nice\" + 0.150*\"quality\" + 0.094*\"price\" + 0.057*\"pretty\" + 0.026*\"sturdy\" + 0.025*\"high\" + 0.024*\"pleased\" + 0.023*\"fast\" + 0.020*\"shipping\"\n",
      "Topic: 5 \n",
      "Words: 0.024*\"get\" + 0.022*\"time\" + 0.014*\"even\" + 0.013*\"back\" + 0.013*\"dont\" + 0.011*\"didnt\" + 0.010*\"review\" + 0.010*\"thing\" + 0.009*\"could\" + 0.009*\"got\"\n",
      "Topic: 6 \n",
      "Words: 0.113*\"came\" + 0.099*\"arrived\" + 0.091*\"box\" + 0.055*\"frame\" + 0.044*\"broken\" + 0.034*\"blanket\" + 0.030*\"bright\" + 0.026*\"packaged\" + 0.023*\"packaging\" + 0.023*\"window\"\n",
      "Topic: 7 \n",
      "Words: 0.278*\"made\" + 0.112*\"bed\" + 0.064*\"comfortable\" + 0.033*\"drawer\" + 0.024*\"office\" + 0.024*\"everyone\" + 0.024*\"furniture\" + 0.022*\"guest\" + 0.020*\"china\" + 0.017*\"built\"\n",
      "Topic: 8 \n",
      "Words: 0.108*\"one\" + 0.048*\"bought\" + 0.034*\"used\" + 0.027*\"year\" + 0.025*\"two\" + 0.021*\"ive\" + 0.021*\"purchased\" + 0.019*\"bag\" + 0.018*\"ordered\" + 0.018*\"new\"\n",
      "Topic: 9 \n",
      "Words: 0.033*\"party\" + 0.032*\"dish\" + 0.031*\"bowl\" + 0.031*\"container\" + 0.031*\"cold\" + 0.031*\"food\" + 0.030*\"ice\" + 0.026*\"dishwasher\" + 0.024*\"birthday\" + 0.023*\"dry\"\n",
      "Topic: 10 \n",
      "Words: 0.114*\"exactly\" + 0.094*\"wanted\" + 0.081*\"needed\" + 0.071*\"rug\" + 0.065*\"smell\" + 0.058*\"described\" + 0.056*\"quickly\" + 0.041*\"bathroom\" + 0.025*\"shipped\" + 0.024*\"decor\"\n",
      "Topic: 11 \n",
      "Words: 0.146*\"pillow\" + 0.096*\"together\" + 0.052*\"case\" + 0.042*\"tree\" + 0.032*\"hanging\" + 0.031*\"cake\" + 0.031*\"mat\" + 0.028*\"spoon\" + 0.025*\"bar\" + 0.025*\"rack\"\n",
      "Topic: 12 \n",
      "Words: 0.100*\"item\" + 0.078*\"recommend\" + 0.071*\"received\" + 0.070*\"purchase\" + 0.042*\"order\" + 0.041*\"would\" + 0.033*\"amazon\" + 0.032*\"seller\" + 0.031*\"highly\" + 0.025*\"product\"\n",
      "Topic: 13 \n",
      "Words: 0.080*\"knife\" + 0.057*\"cut\" + 0.036*\"sharp\" + 0.032*\"steel\" + 0.032*\"board\" + 0.029*\"blade\" + 0.027*\"cutting\" + 0.025*\"stainless\" + 0.022*\"pad\" + 0.015*\"edge\"\n",
      "Topic: 14 \n",
      "Words: 0.034*\"clock\" + 0.031*\"unit\" + 0.030*\"print\" + 0.027*\"fan\" + 0.026*\"battery\" + 0.026*\"air\" + 0.026*\"night\" + 0.022*\"floor\" + 0.021*\"vacuum\" + 0.016*\"living\"\n",
      "Topic: 15 \n",
      "Words: 0.100*\"bottle\" + 0.074*\"space\" + 0.056*\"advertised\" + 0.054*\"door\" + 0.051*\"shelf\" + 0.051*\"blue\" + 0.049*\"thanks\" + 0.039*\"jar\" + 0.031*\"counter\" + 0.030*\"cabinet\"\n",
      "Topic: 16 \n",
      "Words: 0.299*\"easy\" + 0.119*\"clean\" + 0.045*\"use\" + 0.039*\"simple\" + 0.038*\"assemble\" + 0.030*\"easier\" + 0.029*\"tool\" + 0.026*\"tray\" + 0.023*\"cleaning\" + 0.022*\"install\"\n",
      "Topic: 17 \n",
      "Words: 0.133*\"great\" + 0.119*\"love\" + 0.077*\"look\" + 0.054*\"product\" + 0.047*\"color\" + 0.041*\"beautiful\" + 0.029*\"picture\" + 0.027*\"cute\" + 0.021*\"set\" + 0.020*\"happy\"\n",
      "Topic: 18 \n",
      "Words: 0.371*\"work\" + 0.058*\"pan\" + 0.036*\"pictured\" + 0.028*\"pot\" + 0.028*\"oil\" + 0.024*\"cooking\" + 0.022*\"oven\" + 0.022*\"cook\" + 0.021*\"egg\" + 0.019*\"stick\"\n",
      "Topic: 19 \n",
      "Words: 0.040*\"like\" + 0.034*\"well\" + 0.023*\"would\" + 0.022*\"little\" + 0.020*\"size\" + 0.020*\"use\" + 0.019*\"fit\" + 0.019*\"really\" + 0.016*\"small\" + 0.015*\"also\"\n"
     ]
    }
   ],
   "source": [
    "# print topics and their top words\n",
    "for index, topic in lda_model3.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fd239ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"Downloads/topic.model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(lda_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3fa83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
